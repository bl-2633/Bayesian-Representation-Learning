{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Bayesian Neural Networks for semi-supervised Representation Learning and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In recent years we witness plenty of breakthroughs powered by probablistic graphical model and deep learning, especially the power of deep generative models such as GAN or VAE. In this project we mainly explore the performances of autoencoder its counterpart: bayesian autorencoder. Besides learning the deep presentation and visualize the result to see whether the representation learned indeed makes sense, we also use the learned representation to perform classification problems. We aim to test whether these deep learning models indeed outperform their probablistic counterpart and whether edward can successfully infer the parameters in the model. The dataset we are using is MNIST, a widely used standard data set for image classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import all necessary libraries\n",
    "%matplotlib inline\n",
    "from edward.util import Progbar\n",
    "import tensorflow as tf\n",
    "from utils import (generator_xy, generator, \n",
    "                   load_data, accuracy, visulize,)\n",
    "import random\n",
    "import edward as ed\n",
    "import numpy as np\n",
    "from AE import encoder, decoder, mlp, NN_classifier\n",
    "from observations import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from edward.models import Normal, Bernoulli, Categorical, Laplace\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A function that would randomly split the training and test set\n",
    "#Default split proportion 0.8\n",
    "def split_train_test(DATA, y, proportion=0.8):\n",
    "    data_size = len(DATA)\n",
    "    train_size = int(proportion * data_size)\n",
    "    permutation = [i for i in range(data_size)]\n",
    "    random.shuffle(permutation)\n",
    "    train_DATA = DATA[permutation[:train_size]]\n",
    "    train_label = y[permutation[:train_size]]\n",
    "    test_DATA = DATA[permutation[train_size:]]\n",
    "    test_label = y[permutation[train_size:]]\n",
    "    return train_DATA, train_label, test_DATA, test_label\n",
    "\n",
    "#Change the one hot representation to label\n",
    "def categorical(y_one_hot):\n",
    "    return np.argmax(y_one_hot, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we are using are standard MNIST dataset: the data set contains 60,000 training image and testing image, each image is a hand written digit 0~9 (labeled) with size 28 * 28, each pixel being 0 (black) or 1 (white)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed.set_seed(100)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.as_default()\n",
    "M = 12000\n",
    "(x_train, y_train), (x_test, y_test) = mnist('./data')\n",
    "y_train = tf.cast(y_train, tf.int32).eval()\n",
    "y_test = tf.cast(y_test, tf.int32).eval()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train_AE = load_data('data/encoded_train_AE.pkl')\n",
    "x_test_AE = load_data('data/encoded_test_AE.pkl')\n",
    "x_train_BAE = load_data('data/encoded_train_BAE.pkl')\n",
    "x_test_BAE = load_data('data/encoded_test_BAE.pkl')\n",
    "x_train_generator = generator_xy([x_train,y_train], M)\n",
    "x_train_generator_AE = generator_xy([x_train_AE, y_train], M)\n",
    "x_train_generator_BAE = generator_xy([x_train_BAE, y_train], M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_of_class = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visulize(10, x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image of the model goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Logistics Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use bayesian logistics regression as a baseline. The parameters are properly tuned, which is made convenient by edward. Particularly, we are interested in the relation of two model criticisms: accuracy, and the difference between prior and posterior parameter distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input:\n",
    "# qz: the distribution of the input variables, either latent/or not dimensions\n",
    "#y_labels: the labels for this supervised learning\n",
    "#latent_dim: the dimension of qz\n",
    "#regression_coef_var: equivalent to regularization term\n",
    "def bayesian_logistics_regression(qz, y_labels, latent_dim, regression_coef_var, prior, plot_loss=False):\n",
    "    if prior == 'gaussian':\n",
    "        regression_coef = Normal(loc=tf.zeros([num_of_class, latent_dim]), \n",
    "               scale=regression_coef_var * tf.ones([num_of_class, latent_dim]))\n",
    "    elif prior == 'laplacian':\n",
    "        regression_coef = Laplace(loc=tf.zeros([num_of_class, latent_dim]), \n",
    "               scale=regression_coef_var * tf.ones([num_of_class, latent_dim]))\n",
    "    z = tf.placeholder(tf.float32, [qz.shape[0], latent_dim])\n",
    "    y = Categorical(logits=tf.matmul(z, regression_coef, transpose_b=True))\n",
    "    qcoeff = Normal(loc=tf.Variable(tf.random_normal([num_of_class, latent_dim])),\n",
    "            scale=tf.nn.softplus(tf.Variable(tf.random_normal([num_of_class, latent_dim]))))\n",
    "    inference = ed.KLqp({regression_coef: qcoeff}, data={z: qz, y: y_labels})\n",
    "    inference.initialize(n_iter = 1000, n_samples = 5)\n",
    "    tf.global_variables_initializer().run()        \n",
    "    losses = []\n",
    "    for _ in range(100):\n",
    "        info_dict = inference.update(feed_dict={z: qz, y: y_labels})\n",
    "        losses.append(info_dict['loss'])\n",
    "    if plot_loss:\n",
    "        plt.title('inference loss')\n",
    "        plt.xlabel('iteration')\n",
    "        plt.ylabel('loss')\n",
    "        plt.plot(range(100), losses)\n",
    "        plt.show()\n",
    "    print('finished logistics regression')\n",
    "    return qcoeff, regression_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A wrapper for raw bayesian logistics regression\n",
    "#return the area under the roc curve\n",
    "def raw_bayesian_logistics_regression(regression_coef_var, DATA, labels, prior='gaussian', plot_loss=False):\n",
    "    train_DATA, train_label, test_DATA, test_label = split_train_test(DATA, labels)\n",
    "    dimension = DATA.shape[1]\n",
    "    qcoef, coef_prior = bayesian_logistics_regression(train_DATA, train_label, dimension, regression_coef_var\n",
    "                                                      , prior, plot_loss)\n",
    "    n_samples = 100\n",
    "    test_z = tf.placeholder(tf.float32, test_DATA.shape)\n",
    "    probas = tf.reduce_mean(tf.stack([tf.sigmoid(tf.matmul(test_z, \n",
    "                                                           qcoef.sample(), \n",
    "                                                           transpose_b=True)) \n",
    "                                      for _ in range(n_samples)],axis=0), axis=0)\n",
    "    score = probas.eval(feed_dict={test_z: test_DATA})\n",
    "    y_pred = categorical(score)\n",
    "    return accuracy_score(y_pred, test_label), qcoef, coef_prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Bayesian Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define model here\n",
    "num_hidden = 100\n",
    "D = 256\n",
    "num_class = 10\n",
    "\n",
    "W_0 = Normal(loc = tf.zeros([D, num_hidden]), scale = tf.ones([D, num_hidden]))\n",
    "W_1 = Normal(loc = tf.zeros([num_hidden, num_class]), scale = tf.ones([num_hidden,num_class]))\n",
    "b_0 = Normal(loc = tf.zeros(num_hidden), scale = tf.ones(num_hidden))\n",
    "b_1 = Normal(loc = tf.zeros(num_class), scale = tf.ones(num_class))\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, D])\n",
    "\n",
    "y = Categorical(logits=NN_classifier(x, W_0, W_1, b_0, b_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the variational model here\n",
    "qW_0 = Normal(loc = tf.Variable(tf.random_normal([D,num_hidden])),\n",
    "                  scale = tf.nn.softplus(tf.Variable(tf.random_normal([D,num_hidden]))))\n",
    "qW_1 = Normal(loc = tf.Variable(tf.random_normal([num_hidden,num_class])),\n",
    "                  scale = tf.nn.softplus(tf.Variable(tf.random_normal([num_hidden,num_class]))))\n",
    "qb_0 = Normal(loc = tf.Variable(tf.random_normal([num_hidden])),\n",
    "                  scale = tf.nn.softplus(tf.Variable(tf.random_normal([num_hidden]))))\n",
    "qb_1 = Normal(loc = tf.Variable(tf.random_normal([num_class])),\n",
    "                   scale = tf.nn.softplus(tf.Variable(tf.random_normal([num_class]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Inference and Critisism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: bayesian logistics regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to explore the relation between i)accuracy and ii) whether parameter prior-posterior fit each other. We try to find whether the improvement of parameter prior-posterior fitting would increase the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_coef_var = 1\n",
    "acc, qcoef, coef_prior = raw_bayesian_logistics_regression(regression_coef_var, x_train, y_train, plot_loss=True)\n",
    "print('The accuracy achieved by raw bayesian logistics regression is %f' \n",
    "      % acc)\n",
    "\n",
    "pos_encode = qcoef.sample().eval()\n",
    "pri_encode = coef_prior.sample().eval()\n",
    "\n",
    "sns.distplot(np.ndarray.flatten(pos_encode), color = 'red')\n",
    "sns.distplot(np.ndarray.flatten(pri_encode), color = 'blue')\n",
    "plt.title('Prior and Posterior Distribution of Logistic Regression Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_coef_var = 1\n",
    "acc, qcoef, coef_prior = raw_bayesian_logistics_regression(regression_coef_var, x_train, \n",
    "                                                           y_train, prior='laplacian', plot_loss=True)\n",
    "print('The accuracy achieved by raw bayesian logistics regression is %f' \n",
    "      % acc)\n",
    "\n",
    "pos_encode = qcoef.sample().eval()\n",
    "pri_encode = coef_prior.sample().eval()\n",
    "\n",
    "sns.distplot(np.ndarray.flatten(pos_encode), color = 'red')\n",
    "sns.distplot(np.ndarray.flatten(pri_encode), color = 'blue')\n",
    "plt.title('Prior and Posterior Distribution of Logistic Regression Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_coef_var = 1.5\n",
    "acc, qcoef, coef_prior = raw_bayesian_logistics_regression(regression_coef_var, x_train, \n",
    "                                                           y_train, prior='laplacian', plot_loss=True)\n",
    "print('The accuracy achieved by raw bayesian logistics regression is %f' \n",
    "      % acc)\n",
    "\n",
    "pos_encode = qcoef.sample().eval()\n",
    "pri_encode = coef_prior.sample().eval()\n",
    "\n",
    "sns.distplot(np.ndarray.flatten(pos_encode), color = 'red')\n",
    "sns.distplot(np.ndarray.flatten(pri_encode), color = 'blue')\n",
    "plt.title('Prior and Posterior Distribution of Logistic Regression Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can we conclude about the previous graphs and statistics? \n",
    "First and foremost, there seems to be no problem with the inference algorithm: the loss converges nicely.\n",
    "However, there seems to be no relation between parameter prior-posterior fitting and accuracy. Is it really the case? In this project we observe that, across different runs, we observe very different performance. Therefore, more runs are necessary to get a stable comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_fold = 10\n",
    "result_sheet = np.zeros((3, cv_fold))\n",
    "for fold in range(cv_fold):\n",
    "    print('fold = %d' % fold)\n",
    "    regression_coef_var = 1\n",
    "    acc, qcoef, coef_prior = raw_bayesian_logistics_regression(regression_coef_var, x_train, y_train)\n",
    "    result_sheet[0][fold] = acc\n",
    "    regression_coef_var = 1\n",
    "    acc, qcoef, coef_prior = raw_bayesian_logistics_regression(regression_coef_var, x_train, \n",
    "                                                               y_train, prior='laplacian')\n",
    "    result_sheet[1][fold] = acc\n",
    "    regression_coef_var = 1.5\n",
    "    acc, qcoef, coef_prior = raw_bayesian_logistics_regression(regression_coef_var, x_train, \n",
    "                                                               y_train, prior='laplacian')\n",
    "    result_sheet[2][fold] = acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
