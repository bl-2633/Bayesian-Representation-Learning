{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Bayesian Neural Networks for semi-supervised Representation Learning and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ruiqi Zhong: rz2383 SEAS'19\n",
    "Ben Lai: bl2633 SEAS'18\n",
    "We divided our job as follows: Ben was mainly responsible for modeling, inferencing and writing the majority of the code, Ruiqi was mainly responsible for model criticism, generating images and writing up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In recent years we witness plenty of breakthroughs powered by probablistic graphical model and deep learning, especially the power of deep generative models such as GAN or VAE. In this project we mainly explore the performances of autoencoder and semi-supervised bayesian autoencoder; we also compare their results with bayesian neural network. Besides learning the deep presentation and visualize the result to see whether the representation learned indeed makes sense, we also use the learned representation to perform classification problems. We aim to test whether autoencoder outperforms its probablistic counterpart (or other way around) and whether edward can successfully infer the parameters in the model. The dataset we are using is MNIST, a widely used standard data set for image classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import all necessary libraries\n",
    "%matplotlib inline\n",
    "from edward.util import Progbar\n",
    "import tensorflow as tf\n",
    "from utils import (generator_xy, generator, \n",
    "                   load_data, accuracy, visulize,)\n",
    "import edward as ed\n",
    "import numpy as np\n",
    "from AE import encoder, decoder, mlp, NN_classifier\n",
    "from observations import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from edward.models import Normal, Bernoulli, Categorical\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we are using are standard MNIST dataset: the data set contains 60,000 training image and testing image, each image is a hand written digit 0~9 (labeled) with size 28 * 28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ed.set_seed(104)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.as_default()\n",
    "M = 12000\n",
    "(x_train, y_train), (x_test, y_test) = mnist('./data')\n",
    "y_train = tf.cast(y_train, tf.int32).eval()\n",
    "y_test = tf.cast(y_test, tf.int32).eval()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train_AE = load_data('./data/encoded_train_AE.pkl')\n",
    "x_test_AE = load_data('./data/encoded_test_AE.pkl')\n",
    "x_train_BAE = load_data('./data/encoded_train_BAE.pkl')\n",
    "x_test_BAE = load_data('./data/encoded_test_BAE.pkl')\n",
    "x_train_generator = generator_xy([x_train,y_train], M)\n",
    "x_train_generator_AE = generator_xy([x_train_AE, y_train], M)\n",
    "x_train_generator_BAE = generator_xy([x_train_BAE, y_train], M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visulize(10, x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Semi-supervised Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our Bayesian Semi-supervised Autoencoder as follows: \n",
    "\n",
    "\n",
    "$\\Sigma_{1e},\\Sigma_{1d}, \\Sigma, \\sigma$ are constants. x is the image data we observe, y is the label.\n",
    "\n",
    "$W_{1e},W_{1d}, W \\sim Gaussian(0, \\Sigma_{1e}),Gaussian(0, \\Sigma_{1d}), Gaussian(0, \\Sigma)$\n",
    "\n",
    "$z \\sim relu(W_{1e}x), \\hat{x} \\sim relu(W_{1d}x), x \\sim Gaussian(\\hat{x}, \\sigma), y \\sim Categorical(Softmax(Wz))$\n",
    "\n",
    "The graphical model is as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/SemiBAE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For detailed notebook, please go to Bayesian_AE.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model description goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/Bayesian_AE_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/autoencoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image from https://www.doc.ic.ac.uk/~js4416/163/website/autoencoders/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we only use KLqp as our inference method. As inference is not our main focus in this project, we give a very brief and fast introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/Bayesian_AE_inference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criticism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we have two types of model criticism: i) image reconstruction ii) classification accuracy. To get some intuition on how to perforom model criticism rigidly in Probabilistic Programming language setting, we first worked on a toy model criticism: studying the relationship between classification accuracy and the extent of matching between parameter prior and posterior in bayesian logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criticism on a toy example: bayesian logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In brief, the graphical model of bayesian logistics regression is: $W \\sim \\Sigma, y \\sim Categorical(softmax(Wx))$, where x is the image and y the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/BLR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some runs of Bayesian logistic regression on MNIST image. Statistics on: loss over training, accuracy and parameter prior-posterior are collected "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/BLRG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior distribution of the parameters does not seem to capture the heavy tail; switch to laplace distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/BLRLS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to centered, try flattening it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/BLRLF.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Through these runs, can we reach the conclusion that: the closer the prior and posterior, the better the accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some further results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/BLRComparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An important lesson we learned throughout this project: while doing model criticism, always run multiple times to compare the performance, which itself is probablistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criticism on Learned Representation (through visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-supervised Bayesian Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/Bayesian_AE_prior_draw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/Bayesian_AE_posterior_draw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/Bayesian_AE_training_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also build a autoencoder in Keras for comparison purposes. For detailed notebook, please go to AE.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/AE_reconstruct.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criticism On Classification Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the encoded representation of both models (semisupervised bayesian autoencoder) and autoencoder, and then use a two layer bayesian neural network to perform classification and test their respective accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Algorithm: bayesian deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define model here\n",
    "num_hidden = 100\n",
    "D = 256\n",
    "num_class = 10\n",
    "\n",
    "W_0 = Normal(loc = tf.zeros([D, num_hidden]), scale = tf.ones([D, num_hidden]))\n",
    "W_1 = Normal(loc = tf.zeros([num_hidden, num_class]), scale = tf.ones([num_hidden,num_class]))\n",
    "b_0 = Normal(loc = tf.zeros(num_hidden), scale = tf.ones(num_hidden))\n",
    "b_1 = Normal(loc = tf.zeros(num_class), scale = tf.ones(num_class))\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, D])\n",
    "\n",
    "y = Categorical(logits=NN_classifier(x, W_0, W_1, b_0, b_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the variational model here\n",
    "qW_0 = Normal(loc = tf.Variable(tf.random_normal([D,num_hidden])),\n",
    "                  scale = tf.nn.softplus(tf.Variable(tf.random_normal([D,num_hidden]))))\n",
    "qW_1 = Normal(loc = tf.Variable(tf.random_normal([num_hidden,num_class])),\n",
    "                  scale = tf.nn.softplus(tf.Variable(tf.random_normal([num_hidden,num_class]))))\n",
    "qb_0 = Normal(loc = tf.Variable(tf.random_normal([num_hidden])),\n",
    "                  scale = tf.nn.softplus(tf.Variable(tf.random_normal([num_hidden]))))\n",
    "qb_1 = Normal(loc = tf.Variable(tf.random_normal([num_class])),\n",
    "                   scale = tf.nn.softplus(tf.Variable(tf.random_normal([num_class]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy based on Keras Autoencoder Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inferece initilization for AE representation\n",
    "inference = ed.KLqp({W_0:qW_0, b_0:qb_0,\n",
    "                    W_1:qW_1, b_1:qb_1}, \n",
    "                    data = {x: x_train_AE, y: y_train})\n",
    "inference.initialize(n_iter = 1000, n_samples = 5)\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior draw and prediction accuracy for AE representation\n",
    "pri_qW_0 = qW_0.sample()\n",
    "pri_qW_1 = qW_1.sample()\n",
    "pri_qb_0 = qb_0.sample()\n",
    "pri_qb_1 = qb_1.sample()\n",
    "\n",
    "prior_weights = [pri_qW_0, pri_qW_1, pri_qb_0, pri_qb_1] \n",
    "\n",
    "print(\"training accuracy:\")\n",
    "print(accuracy(x_train_AE, y_train, prior_weights))\n",
    "print(\"testing accuracy:\")\n",
    "print(accuracy(x_test_AE, y_test, prior_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 100\n",
    "n_iter_per_epoch = x_train_AE.shape[0] // M\n",
    "loss = []\n",
    "for epoch in range(1, n_epoch + 1):\n",
    "    print(\"Epoch: {0}\".format(epoch))\n",
    "    avg_loss = 0.0\n",
    "\n",
    "    # pbar = Progbar(n_iter_per_epoch)\n",
    "    for t in range(1, n_iter_per_epoch + 1):\n",
    "        # pbar.update(t)\n",
    "        x_batch, y_batch = next(x_train_generator_AE)\n",
    "        info_dict = inference.update(feed_dict={x: x_batch, y: y_batch})\n",
    "        avg_loss += info_dict['loss']\n",
    "        avg_loss = avg_loss / n_iter_per_epoch\n",
    "        avg_loss = avg_loss / M\n",
    "        loss.append(avg_loss)\n",
    "    print(\"-log p(x) <= {:0.3f}\".format(avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loss plot\n",
    "plt.plot(range(5 * n_epoch), loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posterior accuracy check for AE representation\n",
    "pos_qW_0 = qW_0.sample()\n",
    "pos_qW_1 = qW_1.sample()\n",
    "pos_qb_0 = qb_0.sample()\n",
    "pos_qb_1 = qb_1.sample()\n",
    "\n",
    "posterior_weights = [pos_qW_0, pos_qW_1, pos_qb_0, pos_qb_1] \n",
    "\n",
    "print(\"training accuracy:\")\n",
    "print(accuracy(x_train_AE, y_train, prior_weights))\n",
    "print(\"testing accuracy:\")\n",
    "print(accuracy(x_test_AE, y_test, prior_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy based on Semi-supervised Bayesian Autoencoder Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inferece initilization for Bayesian AE representation\n",
    "inference = ed.KLqp({W_0:qW_0, b_0:qb_0,\n",
    "                    W_1:qW_1, b_1:qb_1}, \n",
    "                    data = {x: x_train_BAE, y: y_train})\n",
    "inference.initialize(n_iter = 1000, n_samples = 5)\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior draw and prediction accuracy for Bayesian AE representation\n",
    "pri_qW_0 = qW_0.sample()\n",
    "pri_qW_1 = qW_1.sample()\n",
    "pri_qb_0 = qb_0.sample()\n",
    "pri_qb_1 = qb_1.sample()\n",
    "\n",
    "prior_weights = [pri_qW_0, pri_qW_1, pri_qb_0, pri_qb_1] \n",
    "\n",
    "print(\"training accuracy:\")\n",
    "print(accuracy(x_train_BAE, y_train, prior_weights))\n",
    "print(\"testing accuracy:\")\n",
    "print(accuracy(x_test_BAE, y_test, prior_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 100\n",
    "n_iter_per_epoch = x_train_BAE.shape[0] // M\n",
    "loss = []\n",
    "for epoch in range(1, n_epoch + 1):\n",
    "    print(\"Epoch: {0}\".format(epoch))\n",
    "    avg_loss = 0.0\n",
    "\n",
    "    # pbar = Progbar(n_iter_per_epoch)\n",
    "    for t in range(1, n_iter_per_epoch + 1):\n",
    "        # pbar.update(t)\n",
    "        x_batch, y_batch = next(x_train_generator_BAE)\n",
    "        info_dict = inference.update(feed_dict={x: x_batch, y: y_batch})\n",
    "        avg_loss += info_dict['loss']\n",
    "        avg_loss = avg_loss / n_iter_per_epoch\n",
    "        avg_loss = avg_loss / M\n",
    "        loss.append(avg_loss)\n",
    "    print(\"-log p(x) <= {:0.3f}\".format(avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loss plot\n",
    "plt.plot(range(5 * n_epoch), loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posterior accuracy check for Bayesian AE representation\n",
    "pos_qW_0 = qW_0.sample()\n",
    "pos_qW_1 = qW_1.sample()\n",
    "pos_qb_0 = qb_0.sample()\n",
    "pos_qb_1 = qb_1.sample()\n",
    "\n",
    "posterior_weights = [pos_qW_0, pos_qW_1, pos_qb_0, pos_qb_1] \n",
    "\n",
    "print(\"training accuracy:\")\n",
    "print(accuracy(x_train_BAE, y_train, prior_weights))\n",
    "print(\"testing accuracy:\")\n",
    "print(accuracy(x_test_BAE, y_test, prior_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the above procedures several times to get a stable comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/Comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: semi supervised bayesian auto encoder outperforms auto encoder in classification problem by a stable and healthy margin. We also observe that the loss in semi-supervised Bayesian auto encoder is smoother than that of auto encoder. However, it still remains unknown whether it is the representation or the probabilistic part that is effective. Due to project time limit, we do not have time to run experiments to investigate this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
